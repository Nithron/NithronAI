# nAI Model Router Configuration
# ================================
# LiteLLM-compatible routing configuration for LLM providers

# Default model when none specified
default_model: ollama/llama3.2

# Fallback chain (try in order)
fallback_models:
  - ollama/llama3.2
  - ollama/mistral
  - gpt-3.5-turbo

# Model definitions
models:
  # Local models (Ollama)
  ollama/llama3.2:
    provider: ollama
    api_base: http://localhost:11434
    max_tokens: 4096
    supports_streaming: true
    cost_per_1k_input: 0
    cost_per_1k_output: 0
    
  ollama/mistral:
    provider: ollama
    api_base: http://localhost:11434
    max_tokens: 8192
    supports_streaming: true
    cost_per_1k_input: 0
    cost_per_1k_output: 0
    
  ollama/codellama:
    provider: ollama
    api_base: http://localhost:11434
    max_tokens: 4096
    supports_streaming: true
    use_for: code
    
  # OpenAI models
  gpt-4:
    provider: openai
    max_tokens: 8192
    supports_streaming: true
    cost_per_1k_input: 0.03
    cost_per_1k_output: 0.06
    
  gpt-4-turbo:
    provider: openai
    max_tokens: 128000
    supports_streaming: true
    cost_per_1k_input: 0.01
    cost_per_1k_output: 0.03
    
  gpt-3.5-turbo:
    provider: openai
    max_tokens: 16384
    supports_streaming: true
    cost_per_1k_input: 0.0005
    cost_per_1k_output: 0.0015
    
  # Anthropic models
  claude-3-opus:
    provider: anthropic
    model_name: claude-3-opus-20240229
    max_tokens: 4096
    supports_streaming: true
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075
    
  claude-3-sonnet:
    provider: anthropic
    model_name: claude-3-sonnet-20240229
    max_tokens: 4096
    supports_streaming: true
    cost_per_1k_input: 0.003
    cost_per_1k_output: 0.015
    
  claude-3-haiku:
    provider: anthropic
    model_name: claude-3-haiku-20240307
    max_tokens: 4096
    supports_streaming: true
    cost_per_1k_input: 0.00025
    cost_per_1k_output: 0.00125

# Routing rules
routing:
  # Route by task type
  by_task:
    code_generation:
      models: [ollama/codellama, gpt-4, claude-3-opus]
      prefer_local: true
      
    summarization:
      models: [ollama/mistral, gpt-3.5-turbo, claude-3-haiku]
      prefer_local: true
      
    qa:
      models: [ollama/llama3.2, gpt-4-turbo, claude-3-sonnet]
      prefer_local: true
      
    chat:
      models: [ollama/llama3.2, gpt-3.5-turbo, claude-3-haiku]
      prefer_local: true
      
  # Route by context length
  by_context_length:
    short:  # < 4K tokens
      models: [ollama/llama3.2, gpt-3.5-turbo]
    medium: # 4K - 32K tokens
      models: [ollama/mistral, gpt-4-turbo]
    long:   # > 32K tokens
      models: [gpt-4-turbo, claude-3-opus]

# Rate limiting (per model)
rate_limits:
  ollama/*:
    requests_per_minute: 60
    tokens_per_minute: 100000
  openai/*:
    requests_per_minute: 60
    tokens_per_minute: 90000
  anthropic/*:
    requests_per_minute: 60
    tokens_per_minute: 100000

# Retry configuration
retry:
  max_retries: 3
  retry_delay_seconds: 1
  retry_on:
    - rate_limit
    - timeout
    - server_error
